{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn as skl\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10531628</td>\n",
       "      <td>AMERICAN CHESAPEAKE CLUB CHARITABLE TR</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10547893</td>\n",
       "      <td>ST CLOUD PROFESSIONAL FIREFIGHTERS</td>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10553066</td>\n",
       "      <td>SOUTHSIDE ATHLETIC ASSOCIATION</td>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556103</td>\n",
       "      <td>GENETIC RESEARCH INSTITUTE OF THE DESERT</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EIN                                      NAME APPLICATION_TYPE  \\\n",
       "0  10520599              BLUE KNIGHTS MOTORCYCLE CLUB              T10   \n",
       "1  10531628    AMERICAN CHESAPEAKE CLUB CHARITABLE TR               T3   \n",
       "2  10547893        ST CLOUD PROFESSIONAL FIREFIGHTERS               T5   \n",
       "3  10553066            SOUTHSIDE ATHLETIC ASSOCIATION               T3   \n",
       "4  10556103  GENETIC RESEARCH INSTITUTE OF THE DESERT               T3   \n",
       "\n",
       "        AFFILIATION CLASSIFICATION      USE_CASE  ORGANIZATION  STATUS  \\\n",
       "0       Independent          C1000    ProductDev   Association       1   \n",
       "1       Independent          C2000  Preservation  Co-operative       1   \n",
       "2  CompanySponsored          C3000    ProductDev   Association       1   \n",
       "3  CompanySponsored          C2000  Preservation         Trust       1   \n",
       "4       Independent          C1000     Heathcare         Trust       1   \n",
       "\n",
       "      INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0              0                      N     5000              1  \n",
       "1         1-9999                      N   108590              1  \n",
       "2              0                      N     5000              0  \n",
       "3    10000-24999                      N     6692              1  \n",
       "4  100000-499999                      N   142590              1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data File\n",
    "charity_df = pd.read_csv('charity_data.csv')\n",
    "charity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries: 0\n"
     ]
    }
   ],
   "source": [
    "## Find duplicate entries, if any.\n",
    "print(f\"Duplicate entries: {charity_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EIN                        int64\n",
       "NAME                      object\n",
       "APPLICATION_TYPE          object\n",
       "AFFILIATION               object\n",
       "CLASSIFICATION            object\n",
       "USE_CASE                  object\n",
       "ORGANIZATION              object\n",
       "STATUS                     int64\n",
       "INCOME_AMT                object\n",
       "SPECIAL_CONSIDERATIONS    object\n",
       "ASK_AMT                    int64\n",
       "IS_SUCCESSFUL              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect data types\n",
    "charity_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EIN has 0 null values\n",
      "NAME has 0 null values\n",
      "APPLICATION_TYPE has 0 null values\n",
      "AFFILIATION has 0 null values\n",
      "CLASSIFICATION has 0 null values\n",
      "USE_CASE has 0 null values\n",
      "ORGANIZATION has 0 null values\n",
      "STATUS has 0 null values\n",
      "INCOME_AMT has 0 null values\n",
      "SPECIAL_CONSIDERATIONS has 0 null values\n",
      "ASK_AMT has 0 null values\n",
      "IS_SUCCESSFUL has 0 null values\n"
     ]
    }
   ],
   "source": [
    "# Find null values, if any.\n",
    "for column in charity_df.columns:\n",
    "    print(f\"{column} has {charity_df[column].isnull().sum()} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries: 0\n"
     ]
    }
   ],
   "source": [
    "## Find duplicate entries, if any.\n",
    "print(f\"Duplicate entries: {charity_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop EIN columns and use name as the unique identifiers\n",
    "charity_df = charity_df.drop([\"EIN\"], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME                      19568\n",
       "APPLICATION_TYPE             17\n",
       "AFFILIATION                   6\n",
       "CLASSIFICATION               71\n",
       "USE_CASE                      5\n",
       "ORGANIZATION                  4\n",
       "INCOME_AMT                    9\n",
       "SPECIAL_CONSIDERATIONS        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert all categorical values \n",
    "charity_cat = charity_df.dtypes[charity_df.dtypes ==\"object\"].index.tolist()\n",
    "charity_df[charity_cat].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "         ...  \n",
       "C6100        1\n",
       "C3700        1\n",
       "C2380        1\n",
       "C2190        1\n",
       "C2561        1\n",
       "Name: CLASSIFICATION, Length: 71, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Investigate classification values to determine if binning is required \n",
    "classification_count = charity_df['CLASSIFICATION'].value_counts()\n",
    "classification_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a652ad5588>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAD5CAYAAADx05gdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5Rc5Xnn++/T1fd7q7t1F0hYwkaKjQ2KHB9ijyckAZyJlRzDWMwkwRwYchxYPknWORmYmcV4SFgTMmuOE9s4DjEkmPGKYIjH03aUMCY4tuMLqLENRsKCjiRQIwl1q2/q++2ZP/ZbrVJR1V3dXbuqq/X7rFX0rne/+93vLhX99HvZ7zZ3R0REJE5lxa6AiIisfgo2IiISOwUbERGJnYKNiIjETsFGRERip2AjIiKxK4+zcDO7HvgTIAF8wd3/MG1/FfBF4GrgLPBRdz8e9t0D3AbMAJ9w96dyLPMzwK3uXr/QObJpa2vzrVu3Lvm6RUQuRs8//3yvu7dn2hdbsDGzBPAg8AtAN3DQzDrc/XBKttuAfnffbmb7gAeAj5rZTmAfsAvYCDxtZpeHY7KWaWa7gea0qmQ8x3x137p1K52dnUu+dhGRi5GZvZZtX5zdaHuALnc/6u6TwH5gb1qevcCjYftJ4Fozs5C+390n3P0Y0BXKy1pmCG7/Bfi9HM8hIiIFEmew2QScSHnfHdIy5nH3aWAQaJ3n2PnKvAvocPdTOZ5DREQKJM4xm0yth/S1cbLlyZaeKTi6mW0EbgI+uMR6YGZ3AHcAXHLJJRkOERGRpYqzZdMNbEl5vxk4mS2PmZUDTUDfPMdmS38PsB3oMrPjQK2ZdS1wjgu4+0Puvtvdd7e3ZxzfEhGRJYoz2BwEdpjZNjOrJBrw70jL0wHcErZvBJ7xaGXQDmCfmVWZ2TZgB/BctjLd/W/cfb27b3X3rcCou29f4BwiIlIgsXWjufu0md0FPEU0TfkRdz9kZvcBne7eATwMPBZaIX1EwYOQ7wngMDAN3OnuMwCZylygKhnPISIihWP6I/+tdu/e7Zr6LCKyOGb2vLvvzrRPKwhI0czMOk90nuD7R88WuyoiErNYVxAQmc9ffvc4v/+1w5QZPPXbH2DHuoZiV0lEYqKWjRTFzKzzF985xva19ZSXlfGlZ18vdpVEJEYKNlIUL58aort/jI//s7fx8zvX8rUXT6HxQ5HVS8FGiiI5TnPN9jZ+dns7vcMTHOsdKXKtRCQuCjZSFM8e62Nray3rm6rZs60FgIPH33KvrYisEgo2UhQvnxripzY1AfC29nrqq8o5dHKoyLUSkbgo2EjBDY1P0d0/xhUbGgEwMy5fV89PTp8rcs1EJC4KNlJwr4Sg8o7156c6v319I0dOn9MkAZFVSsFGCu5oTzQRYPva+rm0t6+rZ3Bsip5zE8WqlojESMFGCu61vhESZcbG5pq5tEvb6gB4vW+0WNUSkRgp2EjBvd43xsbmaioS579+l6ypDfsUbERWIwUbKbjXz45w6Zq6C9I2t9RgpmAjslop2EjBvdY3yiWttRekVZUn2NBYrWAjskop2EhBDY5NMTA6NddtlmrLmlpOKNiIrEoKNlJQyWCSKdhcsqZWLRuRVUrBRgrq1OA4wAUz0ZK2rKnlzaEJJqZnCl0tEYlZrMHGzK43syNm1mVmd2fYX2Vmj4f9z5rZ1pR994T0I2Z23UJlmtnDZvaCmb1oZk+aWX1I/5iZ9ZjZj8Lr9jivWeZ3eigKNhuaqt+yb31jlHZmSPfaiKw2sQUbM0sADwI3ADuBm81sZ1q224B+d98OfAp4IBy7E9gH7AKuBz5nZokFyvwdd7/S3d8FvA7clXKex9393eH1hTiuV3JzenCMRJnRVl/1ln3rQwBKBiQRWT3ibNnsAbrc/ai7TwL7gb1pefYCj4btJ4FrzcxC+n53n3D3Y0BXKC9rme4+BBCOrwG07skKdHpwgrUNVSTK7C37ksEm2dUmIqtHnMFmE3Ai5X13SMuYx92ngUGgdZ5j5y3TzP4COA28A/hMSr6PpHSvbclUWTO7w8w6zayzp6cn54uUxXlzaJx1jW/tQoPzweZNBRuRVSfOYPPWP13f2trIlmex6dGG+63ARuBl4KMh+avA1tC99jTnW1IXFuL+kLvvdvfd7e3tmbJIHpweGp8bm0nXUFVObWVCLRuRVSjOYNMNpLYiNgMns+Uxs3KgCeib59gFy3T3GeBx4CPh/Vl3T444/zlw9ZKvSJbt9OD4XAsmnZmxvrGaNzVmI7LqxBlsDgI7zGybmVUSDfh3pOXpAG4J2zcCz3i0xnwHsC/MVtsG7ACey1amRbbD3JjNLwM/Ce83pJzvw0StHimC4YlphiemswYbiLrSNEFAZPUpj6tgd582s7uAp4AE8Ii7HzKz+4BOd+8AHgYeM7MuohbNvnDsITN7AjgMTAN3hhYLWcosAx41s0airrYXgI+HqnzCzD4cyukDPhbXNcv8zoQgsq7xrTPRktY3VvPsMT0eWmS1iS3YALj7AeBAWtq9KdvjwE1Zjr0fuD/HMmeBa7KUcw9wz2LrLvnXOzwJkHHac1J7YxU95yZwd6JGqoisBlpBQArm7HA0dNZalz3YtNVVMTkzy9D4dKGqJSIFoGAjBdM7Elo2DZVZ8yT39Q5rFQGR1UTBRgom2bJZUztPsAldbL16PLTIqqJgIwXTOzxBS20F5YnsX7u5YBPGd0RkdVCwkYI5OzxJ6zyTAyA12KhlI7KaKNhIwZwdnqStPnsXGsCaukrKTMFGZLVRsJGC6R2ZWLBlkygz1tRVqhtNZJVRsJGCOTs8SVvd/C0biLrS1LIRWV0UbKQgJqdnGRybmveGziQFG5HVR8FGCqIv3GOzUDcaQFt9pYKNyCqjYCMFkQwerQtMEIjyVNF7TmM2IquJgo0UxNnk6gE5BJu2+irGpmYYmdCSNSKrhYKNFERyRYDcxmy0ZI3IaqNgIwVxdiTZjbZwsFkTZqz1j07FWicRKRwFGymIs8OTVJWXUVeZWDBvSzLYjGjcRmS1ULCRgugdnqStviqnZ9QkF+rsU7ARWTViDTZmdr2ZHTGzLjO7O8P+KjN7POx/1sy2puy7J6QfMbPrFirTzB42sxfM7EUze9LM6hc6hxRO/+gkLXUVOeWda9mMKtiIrBaxBRszSwAPAjcAO4GbzWxnWrbbgH533w58CnggHLuT6BHRu4Drgc+ZWWKBMn/H3a9093cBrwN3zXcOKaz+0Ula5nm0QKrG6nISZaaWjcgqEmfLZg/Q5e5H3X0S2A/sTcuzF3g0bD8JXGtRP8teYL+7T7j7MaArlJe1THcfAgjH1wC+wDmkgAZGp2jOMdiYGS21lWrZiKwicQabTcCJlPfdIS1jHnefBgaB1nmOnbdMM/sL4DTwDuAzC5xDCmhgdJLmmty60QDW1FXQP6LZaCKrRZzBJlPrwXPMs9j0aMP9VmAj8DLw0UXUAzO7w8w6zayzp6cnwyGyVLOzzuDYFC21uQebltpK+tSyEVk14gw23cCWlPebgZPZ8phZOdAE9M1z7IJluvsM8DjwkQXOQdpxD7n7bnff3d7envNFysKGxqeYdXLuRoPoXhtNfRZZPeIMNgeBHWa2zcwqiQb8O9LydAC3hO0bgWfc3UP6vjCTbBuwA3guW5kW2Q5zYza/DPxkgXNIgSRvzsx1NlqUV2M2IqtJeVwFu/u0md0FPAUkgEfc/ZCZ3Qd0unsH8DDwmJl1EbU29oVjD5nZE8BhYBq4M7RYyFJmGfComTUSdZu9AHw8VCXjOaRwBkLQaK5ZRMumtpL+0SlmZ52yMs3nECl1sQUbAHc/ABxIS7s3ZXscuCnLsfcD9+dY5ixwTZZysp5DCmMgtGyaFzNmU1fJzKxzbnyapkUcJyIrk1YQkNglu8Nyvc8myhsFGE0SEFkdFGwkdnNjNosJNnVaskZkNVGwkdgNjE5SZtBQnXuvbXJ9NM1IE1kdFGwkdgOjUzTVVCxqoD/5mAF1o4msDgo2ErvFrIuWpMcMiKwuCjYSu4HRqUXPKKurTFCZKNMD1ERWCQUbid1SWjZmRktdhVo2IquEgo3ELlrxefH3ymh9NJHVQ8FGYjewhJYNaH00kdVEwUZiNTk9y8jkzKIeL5DUUqeWjchqoWAjsZpbF61uCS2b2krd1CmySijYSKwGxpKrByxlzKaCwbEpZma1SLdIqVOwkVglx1yWMmbTXFuJOwyNafqzSKlTsJFYJe+TaVrCmI1WERBZPRRsJFbJMZuWJYzZJKdLDyjYiJQ8BRuJ1fLGbJJL1qgbTaTUKdhIrPpHJ6ksL6OmIrHoY9WNJrJ6xBpszOx6MztiZl1mdneG/VVm9njY/6yZbU3Zd09IP2Jm1y1Uppl9KaS/ZGaPmFlFSP+gmQ2a2Y/C616kYAZGpmiuqcBs8Y92VjeayOoRW7AxswTwIHADsBO42cx2pmW7Deh39+3Ap4AHwrE7gX3ALuB64HNmlligzC8B7wDeCdQAt6ec59vu/u7wui//VyvZLGVdtKT6qnIqEqbFOEVWgThbNnuALnc/6u6TwH5gb1qevcCjYftJ4FqL/gTeC+x39wl3PwZ0hfKylunuBzwAngM2x3htkqOBsaWtiwbRYpzNtVqyRmQ1iDPYbAJOpLzvDmkZ87j7NDAItM5z7IJlhu6zXwf+LiX5fWb2gpn9rZntylRZM7vDzDrNrLOnpye3K5QFLXVdtKSW2gr61Y0mUvLiDDaZOunTbwXPlmex6ak+B3zL3b8d3v8AuNTdrwQ+A3wlU2Xd/SF33+3uu9vb2zNlkSXoX+KKz0kttZWajSayCsQZbLqBLSnvNwMns+Uxs3KgCeib59h5yzSz/wi0A7+bTHP3IXcfDtsHgAoza1vOhUlu3J2B0Umal9WyqVTLRmQViDPYHAR2mNk2M6skGvDvSMvTAdwStm8EngljLh3AvjBbbRuwg2gcJmuZZnY7cB1ws7vPJk9gZuvDOBBmtofoms/GcsVygdHJGaZmfEn32CS11FVogoDIKlAeV8HuPm1mdwFPAQngEXc/ZGb3AZ3u3gE8DDxmZl1ELZp94dhDZvYEcBiYBu509xmATGWGU34eeA34XogtXw4zz24EPm5m08AYsC8ENIlZskWy3G60gdFJ3H1J06dFZGWILdjAXLfVgbS0e1O2x4Gbshx7P3B/LmWG9IzX4u6fBT67qIpLXgyEFslyu9GmZ51zE9M0Vi89aIlIcWkFAYlNsmWzrNloYRWBAU0SEClpCjYSm2TLZlljNuFYLVkjUtoUbCQ2yWVmmpYRbJJdcJqRJlLaFGwkNnNjNjVL70ZLLsapVQRESpuCjcSmf3SK+qpyKsuX/jVLdqNp+rNIaVOwkdgMjE0u6QmdqRqrKygzrfwsUupyCjZm9tdm9ktmpuAkORsYnaKlbnnBpqwsWoyzT91oIiUt1+Dxp8C/Al41sz80s3fEWCdZJQZGJ5c1XpPUXFsxN/4jIqUpp2Dj7k+7+78GrgKOA183s++a2a3Jh5SJpBtY5iKcSWvUshEpeTl3i5lZK/AxooeS/RD4E6Lg8/VYaiYlbznPsknVrMU4RUpeTsvVmNmXiZ6C+Rjwy+5+Kux63Mw646qclK7ZWV/2s2ySWmoreOkNdaOJlLJc10b7QliTbI6ZVYUnae6OoV5S4s5NTDPrLHs2GkT32vRpMU6RkpZrN9ofZEj7Xj4rIqvLQB7WRUtqrq1kcnqWsamZZZclIsUxb8vGzNYTPXa5xszew/knZTYCtTHXTUrY+RWfl9+ySb2xs7Yy1oXKRSQmC/2fex3RpIDNwP+fkn4O+Hcx1UlWgfPPssnDmE3KkjWbmmuWXZ6IFN68wcbdHwUeNbOPuPtfF6hOsgoMjuWzZaPFOEVK3bxjNmb2a2Fzq5n9bvprocLN7HozO2JmXWZ2d4b9VWb2eNj/rJltTdl3T0g/YmbXLVSmmX0ppL9kZo8k7/+xyKdD/hfN7KoFPxVZtuTCmfkYs1kTViHQvTYipWuhCQJ14Wc90JDhlZWZJYAHgRuAncDNZrYzLdttQL+7bwc+BTwQjt1J9IjoXcD1wOfMLLFAmV8imp79TqCG6H4gQt4d4XUH0WoIErOB0LJprF7+GEuyK06rCIiUroW60f4s/PxPSyh7D9Dl7kcBzGw/sBc4nJJnL/DJsP0k8FmL5rbuBfa7+wRwzMy6QnlkKzN1araZPUc0zpQ8xxfd3YHvm1mzmW1IuVdIYjAwOkVjdTnlieUvp9dck5wgoJaNSKnKdSHOPzKzRjOrMLO/N7PelC62bDYBJ1Led4e0jHncfRoYBFrnOXbBMkP32a8Df7eIekieDYxO5mVyAEB5oozG6nI900akhOX6Z+cvuvsQ8C+IfllfDvx/CxyT6e47zzHPYtNTfQ74lrt/exH1wMzuMLNOM+vs6enJcIgsRv/o1LIeB52upa5Sz7QRKWG5Bpvkb40PAX/l7n05HNMNbEl5vxk4mS2PmZUDTUDfPMfOW6aZ/UegHUidvJBLPXD3h9x9t7vvbm9vz+HyZD4DY1M05allA1ofTaTU5RpsvmpmPwF2A39vZu3A+ALHHAR2mNk2M6skGvDvSMvTAdwStm8EngljKx3AvjBbbRvR4P5z85VpZrcT3Rd0s7vPpp3jN8KstJ8BBjVeE79oXbT8tWzW1FYo2IiUsJymCrn73Wb2ADDk7jNmNkI08D7fMdNmdhfwFJAAHnH3Q2Z2H9Dp7h3Aw8BjYQJAH1HwIOR7gmgywTRwp7vPAGQqM5zy88BrwPfC+llfdvf7gANELbIuYBS4NZdrluUZGJ2aG9jPh5baSl55czhv5YlIYS1mXuoVRPfbpB7zxfkOCDPEDqSl3ZuyPQ7clOXY+4H7cykzpGe8ltBSunO+ekp+zcw6Q+NTeZsgAMkxG7VsREpVro8YeAx4G/AjILkaorNAsJGL09DYFO75WT0gqaW2gtHJGSamZ6gqT+StXBEpjFxbNruBnaGVIDKv5A2d+Vg9ICn1xs51jQo2IqUm1wkCLwHr46yIrB7J7q6mfE4QCItxaskakdKUa8umDTgc7syfSCa6+4djqZWUtMHROFo2WkVApJTlGmw+GWclZHWZe7xAnmejgdZHEylVuU59/qaZXQrscPenzayWaOqxyFsMxNCyUTeaSGnLdW20f0O0UOafhaRNwFfiqpSUtoHRScoMGvKw4nNSshttQN1oIiUp1wkCdwLXAEMA7v4qsDauSklpGxiboqmmgrKyTMvSLU1VeYK6ygR9I+pGEylFuQabCXef+5My3NipadCSUf9ofm/oTGqurVTLRqRE5Rpsvmlm/w6oMbNfAP478NX4qiWlLHq8QP4mByS11Gl9NJFSlWuwuRvoAX4M/CbRcjH/Ia5KSWnL97poSS21lfRpNppIScp1NtqsmX0F+Iq762EvMq++kUl2rK3Pe7kttZW83jea93JFJH7ztmzCsvyfNLNe4CfAETPrMbN75ztOLm79o5NzU5XzqaW2Qk/rFClRC3Wj/TbRLLSfdvdWd18DvBe4xsx+J/baSckZn5phdHKGNfUxBJu6SobGp5memV04s4isKAsFm98gehjZsWSCux8Ffi3sE7lA8qbLNTHMRptbRWBM4zYipWahYFPh7r3piWHcJv8jwFLy5oJNHN1ooUx1pYmUnoWCzXz/V+v/eHmLOINNsrWkJWtESs9CweZKMxvK8DoHvHOhws3sejM7YmZdZnZ3hv1VZvZ42P+smW1N2XdPSD9iZtctVKaZ3RXS3MzaUtI/aGaDZvaj8NLkhhjFGWxawzhQ77CCjUipmXfqs7svebFNM0sADwK/AHQDB82sw90Pp2S7Deh39+1mtg94APiome0E9gG7gI3A02Z2eTgmW5nfAb4G/EOG6nzb3f/FUq9FchdnsGmrrwKgd3higZwistLkelPnUuwButz9aFjqZj+wNy3PXuDRsP0kcK2ZWUjf7+4TYXJCVygva5nu/kN3Px7j9UgO+kYmSZQZjdX5H9JbU1dJmcFZBRuRkhNnsNkEnEh53x3SMuZx92lgEGid59hcyszkfWb2gpn9rZntypTBzO4ws04z6+zp0X2rS3V2ZJKW2sq8LsKZlCgz1tRV0qNuNJGSE2ewyfTbJn3xzmx5Fps+nx8Al7r7lcBnyPJoBHd/yN13u/vu9vb2BYqUbPpHJllTF99Exda6KnWjiZSgOINNN7Al5f1m4GS2PGEl6Sagb55jcynzAu4+5O7DYfsAUJE6gUDyq28kntUDktoaKhVsREpQnMHmILDDzLaZWSXRgH9HWp4O4JawfSPwjLt7SN8XZqttA3YAz+VY5gXMbH0YB8LM9hBd89m8XKG8RV9MS9UktdWrZSNSivL3KMU07j5tZncBTxE9QvoRdz9kZvcBne7eATwMPGZmXUQtmn3h2ENm9gRwGJgG7nT3GYimOKeXGdI/AfwesB540cwOuPvtREHs42Y2DYwB+0JAkxjE3rKpr+KsxmxESk5swQbmuq0OpKXdm7I9DtyU5dj7gftzKTOkfxr4dIb0zwKfXWzdZfFmZp2B0clYlqpJaq2vZHRyhtHJaWorY/36ikgexdmNJheZwbEpZj2ee2yS5u61OafWjUgpUbCRvOkbicZSWmIMNu0h2PRo3EakpCjYSN70jUSrMbfWVcV2Dq0iIFKaFGwkb5Itmzi70ZLro2mSgEhpUbCRvEkukNkaw4PTks4vxqmWjUgpUbCRvOk5N4EZtMbYsqkqT9BYXa5gI1JiFGwkb3qGJ1hTW0l5It6vVVuDbuwUKTUKNpI3vecmaG+Ib3JAUlt9FT3nFGxESomCjeRNz/DE3GyxOK1rrObNIQUbkVKiYCN501Ogls36xipOD42jVYdESoeCjeSFu9M7XJhgs66xmsnpWQbHpmI/l4jkh4KN5MXwxDTjU7O0xTjtOWl9UzUAp4fGYz+XiOSHgo3kRXLAvjDdaCHYDCrYiJQKBRvJi+QNne311bGfa10INmc0SUCkZCjYSF4kWzZtDfF3o61tjFpP6kYTKR0KNpIXPeeiX/ztBZj6XFWeYE1dpYKNSAmJNdiY2fVmdsTMuszs7gz7q8zs8bD/WTPbmrLvnpB+xMyuW6hMM7srpLmZtaWkm5l9Oux70cyuiu+KL149wxMkyoyWGB+clmpdYzVvasxGpGTEFmzMLAE8CNwA7ARuNrOdadluA/rdfTvwKeCBcOxOokdE7wKuBz5nZokFyvwO8PPAa2nnuAHYEV53AH+az+uUSO+5SVrrKikrs4Kcb31jFW+eU7ARKRVxtmz2AF3uftTdJ4H9wN60PHuBR8P2k8C1ZmYhfb+7T7j7MaArlJe1THf/obsfz1CPvcAXPfJ9oNnMNuT1SoWeAt1jk7SusZrTg5ogIFIq4gw2m4ATKe+7Q1rGPO4+DQwCrfMcm0uZS6mHLFOhVg9IWtdYzdmRCaZmZgt2ThFZujiDTab+lPT1RbLlWWz6cuuBmd1hZp1m1tnT07NAkZKu59xEQSYHJK1vqsYdLcgpUiLiDDbdwJaU95uBk9nymFk50AT0zXNsLmUupR64+0Puvtvdd7e3ty9QpKSanpnlzLlxNjTFf49N0tyNnZqRJlIS4gw2B4EdZrbNzCqJBvw70vJ0ALeE7RuBZzxaXbED2Bdmq20jGtx/Lscy03UAvxFmpf0MMOjup/JxgRLpGZ5g1mF9U03BzrmhOQo2JwfGCnZOEVm68rgKdvdpM7sLeApIAI+4+yEzuw/odPcO4GHgMTPrImrR7AvHHjKzJ4DDwDRwp7vPQDTFOb3MkP4J4PeA9cCLZnbA3W8HDgAfIppkMArcGtc1X6xOhSnIhWzZbGqOAlt3v4KNSCmILdgAuPsBol/2qWn3pmyPAzdlOfZ+4P5cygzpnwY+nSHdgTsXW3fJXXKNsvUFDDYN1RU011bQ3T9asHOKyNJpBQFZtmK0bAA2t9SoZSNSIhRsZNlOD45RVV5GU01FQc+7ublWwUakRCjYyLKdGoxmokX34xZO1LIZ1RM7RUqAgo0s2+nB8YKO1yRtbqlhfGqWsyOTBT+3iCyOgo0sW9SyKdy056RNLbWAZqSJlAIFG1mW2VnnzaHitWwAzUgTKQEKNrIsvcMTTM96wWeiwflgc6JPLRuRlU7BRpblRGhVbAldWoXUUF1Ba10lx3tHCn5uEVkcBRtZltf7QrBZU/gxG4DL2us4pmAjsuIp2MiyJLuwNhehZQOwra2Oowo2Iiuego0sy+t9o6xtqKK6IlGU81/WXk/v8ARD41NFOb+I5EbBRpblRN8ol6wpTqsGopYNwLEetW5EVjIFG1mW7v4xthQx2FyWDDbqShNZ0RRsZMkmp2c5OVjcYHNJay1lhsZtRFY4BRtZspMDY7jDlpbizEQDqCpPsLmlln/qGS5aHURkYQo2smTnpz0Xr2UDcPm6el45fa6odRCR+SnYyJIdDa2J5LhJsVyxoZGjvSOMT80UtR4ikl2swcbMrjezI2bWZWZ3Z9hfZWaPh/3PmtnWlH33hPQjZnbdQmWa2bZQxquhzMqQ/jEz6zGzH4XX7XFe88Wkq2eYhupy2huqilqPd6xvZGbWefVNdaWJrFSxBRszSwAPAjcAO4GbzWxnWrbbgH533w58CnggHLsT2AfsAq4HPmdmiQXKfAD4lLvvAPpD2UmPu/u7w+sLMVzuRanrzDDb19YX/Dk26a7Y0ADAy6eGiloPEckuzpbNHqDL3Y+6+ySwH9iblmcv8GjYfhK41qLfXHuB/e4+4e7HgK5QXsYywzE/F8oglPkrMV6bAF1nRtjeXl/sanBpax01FQkOK9iIrFhxBptNwImU990hLWMed58GBoHWeY7Nlt4KDIQyMp3rI2b2opk9aWZbMlXWzO4ws04z6+zp6cn9Ki9Sg6NT9A5PsGNd8YNNosx4+/oGtWxEVrA4g02mvpX05/dmy5OvdICvAlvd/V3A05xvSV2Y2f0hd9/t7rvb29szZZEUXT3R7K/ta4sfbAB2bmzk8MkhZmf1iGiRlSjOYNMNpLYiNgMns+Uxs3KgCeib59hs6b1AcyjjgnO5+1l3nwjpfw5cvayrEiAar5PbJTYAAA9DSURBVAHY3t5Q5JpE3rOlmXMT03TpfhuRFSnOYHMQ2BFmiVUSDfh3pOXpAG4J2zcCz7i7h/R9YbbaNmAH8Fy2MsMx3whlEMr8nwBmtiHlfB8GXs7zdV6UjpweprqijE1FvKEz1e6tawDoPN5f5JqISCaxBZswfnIX8BTRL/gn3P2Qmd1nZh8O2R4GWs2sC/hd4O5w7CHgCeAw8HfAne4+k63MUNa/BX43lNUaygb4hJkdMrMXgE8AH4vrmi8mh04OcsWGRhJlxZ2JlrS1tZbWukqef03BRmQlKl84y9K5+wHgQFravSnb48BNWY69H7g/lzJD+lGi2Wrp6fcA9yy27pLd7Kxz6OQQv/qe9PkexWNmXH1pC8+/1lfsqohIBlpBQBbttb5Rhiem+alNjcWuygWuvrSF42dH6Tk3sXBmESkoBRtZtJfeGARg18amItfkQtdsbwPg269q6rrISqNgI4v2w9cHqCov4/J1K2MmWtLODY20N1TxjSMKNiIrjYKNLNrB4328e0szleUr6+tTVmZ88PJ2vnnkDNMzs8WujoikWFm/LWTFG56Y5tDJQfZsW1PsqmT0z9+xlqHxaX7w+kCxqyIiKRRsZFF+8Fo/sw4/vXVlBpv372ijqryMr76Qfv+wiBSTgo0syrdf7aEiEU0zXokaqiu4btd6Ol44ycS0nm8jslIo2MiifONID+/d1kpdVay3aC3L/3nVJgbHpnjm5TPFroqIBAo2krMTfaN0nRnmn79jbbGrMq/372hnQ1M1f/nd48WuiogECjaSs7/58SkAfv6KlR1sEmXG7e+/jGeP9XHwuFYUEFkJFGwkZ1/54Ru855JmLm2tK3ZVFnTzni2sqavkj59+hWidVhEpJgUbycmPuwf5yelzK2o9tPnUVpbziZ/bzne6ztKhmWkiRadgIzn5wj8epb6qnF8pkWAD8Ovv28qVm5u476uHOTkwVuzqiFzUFGxkQUd7hvnai6fY99NbaKyuKHZ1cpYoM/7rv7ySyelZbnu0k6HxqWJXSeSitXLnr8qK4O78wd+8TE1Fgt/8Z28rdnUWbfvaBj7zr97D7Y928i8//z3+/Dd2s2VN7aLL6R2e4JXT53j1zDCnBsc5OzzBufFpAMrKoLm2kvb6KrasqWXnhka2r61fccv5iBSTgo3M6/GDJ3jmJ2f4D790Be0NVcWuzpJ88O1r+ctb9/Dx//Y8v/ipb/FvPnAZv/beS1jbWJ0xf//IJC90D/DCiUFe6B7gxe4Beocn5/ZXJIzWuioaa8oxjBl3+kcmOTtyYZ53bmrimu1t/B9va+OqS5upKk/Efq0iK5XFOVPHzK4H/gRIAF9w9z9M218FfBG4GjgLfNTdj4d99wC3ATPAJ9z9qfnKDI+P3g+sAX4A/Lq7T853jmx2797tnZ2dy77+UvfMT97kNx97np+5rJW/vHXPinkq51K9MTDGfV89xFOH3gTg8nX1bG2to766nImpWXrOTXD87AhnwvNwzGB7ez3v2tzMzo2NvH1dA5evq6e9oQqzt34Wk9OzvN43yuFTQxw6OcizR/t4sXuAWYeaigTvvWwN79/Rzgd2tLF9bX3GMmRpBkYnOXRyiJfeGKTrzDA9wxP0jUwyM+skyozqigTrG6vZ0FTNjnUNXLGhge1r6/UHQJ6Z2fPuvjvjvriCjZklgFeAXwC6gYPAze5+OCXPbwHvcvf/28z2Ab/q7h81s53AXxE9eXMj8DRweTgsY5lm9gTwZXffb2afB15w9z/Ndo756n6xB5vRyWk+/82jfPaZV9m1sYn/dvt7aaopnbGahfxTzzB/99JpOo/38cbAGCMTM1RVlLGmtpKtbXVsX1vPuzY38c5NTTQsc4xqaHyKZ4/28Z2uXr71ag9He0YAWN9Yzft3tPGzO6KWT6m2GgvN3XlzaIKXQ0B/6Y0hXjo5SHf/+QkgaxuqWNtYRWtdFRUJY9ajBWTfHBrn1OA4k9PRiuDlZcaOdQ28c1MjP7WpiV0bm9i5oZGaSgWgpSpWsHkf8El3vy68vwfA3f9zSp6nQp7vmVk5cBpoB+5OzZvMFw57S5nAHwI9wHp3n049d7Zz+DwXfjEFG3dncGyKM+ei/4GfO9bHgR+fon90il9590b+4FffSf0KXpqm1HT3j/KPr/by7Vd7+ceuXgbHokkLaxuq5lpPm1pq2NhUw4bmapprK6mvKqe+qrzkW5a5mJie4dz4NMPj0/SNTnJyYIyTA2O80T/Gq2eGefnUEP2j5yd6bGurY9fGKFj81MYmdm1spKWuMmv5M7POsd4RXj41FFqgUWuoL3SBlhlsX1vPro1NXNpay8bmGjY319DWUEVDdTmN1RXUVibUKs1ivmAT52+RTcCJlPfdwHuz5QlBYhBoDenfTzs2Oec2U5mtwIC7T2fIn+0cvUu+siy++UoPv/+1w3M3Efrcf+Z+4O4p28l9fn47LQQumJ/04zLty1BGSBubnGF69vxJayoSXHvFWm69ZitXX7oyV3YuZZtbatm35xL27bmEmVnnpTcGOXi8j8Onhjh8cojvdp1lMsuzeGorE1QkyigvMxJlFv1MGOVlZeT6uy/XX5G5/DJNfofco+1ZP//dTKY5MOtR2mz4HyLaPn/srDs4TEzPZr32hupytrXV8Ys713PFhgau2NDIzo2Ni255JsqM7Wvr2b62nl++cuPcdZwaHOelNwZ5KQSf7/3TWf7HD9/IWkZ1eRmJMqMicf5necIoM8v8GWf5OLN9ypk+/0KFt4/+9BZuf/9leS83zmCT6bNJb01ky5MtPdP0nvny51oPzOwO4A6ASy65JMMhC6uvKuftyadX2vkfyS9OsiJmqdsX7sMg+XW1C8pIbp/fZ6kH5ZI/7ZwANZUJ2uqraKuvZMfaaEyiPKFZVIWQKDOu3NLMlVua59JmZ53ekQlODoxzamCMofEpzo1PR3/tT0wzM+tMz85GP2ecmVlnatZzWiUh5z6MHDI6jmHhe2iU2fnvuoXvsBkh3SgrA9LSou3z38+q8gQN1eVzr6aaCjY217CxuSbWKfdmNneeX9y1fi59YnqG04PjvNE/xtmRyfDvMMXQ+BQTU7NMzzpTM9G/xdSMMzM7y0yGzy7bv03WjzlTGbn/6y1bW308XbpxBptuYEvK+81A+q3cyTzdoYurCehb4NhM6b1As5mVh9ZNav5s57iAuz8EPARRN9qirjS4+tKWFbv0vpSGsjJjbUM1axuqeXdKEJLCqypPcGlrXUksz1QK4vwT9iCww8y2mVklsA/oSMvTAdwStm8EngljKR3APjOrCrPMdgDPZSszHPONUAahzP+5wDlERKRAYmvZhPGRu4CniKYpP+Luh8zsPqDT3TuAh4HHzKyLqLWxLxx7KMwuOwxMA3e6+wxApjLDKf8tsN/M/gD4YSibbOcQEZHCifU+m1J1Mc1GExHJl/lmo2kkWEREYqdgIyIisVOwERGR2CnYiIhI7BRsREQkdpqNloGZ9QCvpSS1EcPyNjFSfeNTSnUF1Tduqu+FLnX39kw7FGxyYGad2abzrUSqb3xKqa6g+sZN9c2dutFERCR2CjYiIhI7BZvcPFTsCiyS6hufUqorqL5xU31zpDEbERGJnVo2IiISu4sy2JjZTWZ2yMxmzWx32r57zKzLzI6Y2XUp6deHtC4zuzslfZuZPWtmr5rZ4+HRB4THIzwe8j9rZlvzVPdPmtkbZvaj8PpQvuteKNnqVQxmdtzMfhw+086QtsbMvh4+n6+bWUtINzP7dKj3i2Z2VUo5t4T8r5rZLdnOt4T6PWJmZ8zspZS0vNXPzK4O198Vjl3WgyGz1HdFfnfNbIuZfcPMXg6/F/6fkL4iP9956rsiP9857n7RvYArgLcD/wDsTknfCbwAVAHbgH8iepRBImxfBlSGPDvDMU8A+8L254GPh+3fAj4ftvcBj+ep7p8E/t8M6Xmre4H+DbLWq0jfieNAW1raHwF3h+27gQfC9oeAvyV6OOXPAM+G9DXA0fCzJWy35Kl+HwCuAl6Ko35Ez4t6Xzjmb4EbYqjvivzuAhuAq8J2A/BKqNOK/Hznqe+K/HyTr4uyZePuL7v7kQy79gL73X3C3Y8BXcCe8Opy96PuPgnsB/aGv05+DngyHP8o8CspZT0atp8Erl3uX4sLyGfdCyFjvQp4/lyk/hum/9t+0SPfJ3pK7AbgOuDr7t7n7v3A14Hr81ERd/8Wb33CbF7qF/Y1uvv3PPrt8kWW+V3IUt9sivrddfdT7v6DsH0OeBnYxAr9fOepbzYr4nfDRRls5rEJOJHyvjukZUtvBQY8ehR1avoFZYX9gyF/PtwVmu+PJJv2ea57IWSrV7E48L/M7HkzuyOkrXP3UxD9Dw6sDemL/azjkq/6bQrb6elxWNHfXYu6u98DPEsJfL5p9YUV/Pmu2mBjZk+b2UsZXvP99Zyp5eFLSJ+vrAUtUPc/Bd4GvBs4BfzXGOpeCMU+f7pr3P0q4AbgTjP7wDx5V+pnmrRSvwsr+rtrZvXAXwO/7e5D82VdZL0KVd8V/fnG9ljoYnP3n1/CYd3AlpT3m4GTYTtTei9RE7o8/BWQmj9ZVreZlQNN5NitkGvdzezPga/FUPdCmK++BefuJ8PPM2b2P4i6GN40sw3ufip0hZwJ2bPVvRv4YFr6P8RY7XzVrztsp+fPK3d/M7m90r67ZlZB9Iv7S+7+5ZC8Yj/fTPVdyZ8voYIX7Yu3ThDYxYUDaUeJBtHKw/Y2zg+k7QrH/HcuHEj7rbB9JxdOEHgiT3XekLL9O0R9sXmte4E++6z1KsL3oA5oSNn+LtFYy3/hwgHiPwrbv8SFA8TPhfQ1wDGiweGWsL0mj/XcyoUD7nmrH3Aw5E0OYH8ohvquyO9uuOYvAn+clr4iP9956rsiP9+5OuXrf4RSegG/ShTtJ4A3gadS9v17ohkaR0iZMUI0A+WVsO/fp6RfRjTTpCv8A1WF9OrwvivsvyxPdX8M+DHwItCR9gXLS90L+O+QsV5F+D5cFv5HewE4lKwLUd/13wOvhp/JXxwGPBjq/WMu/IPl/wqfZxdwax7r+FdEXSNT4bt7Wz7rB+wGXgrHfJZww3ee67siv7vAzxJ1E70I/Ci8PrRSP9956rsiP9/kSysIiIhI7FbtBAEREVk5FGxERCR2CjYiIhI7BRsREYmdgo2IiMROwUZERGKnYCMiIrFTsBERkdj9b4znsg9uEJbbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Due to the uneven distribution of CLASSIFICATION, will bucket the rare values into \"other\" category \n",
    "classification_count.plot.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "Other     2261\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on density map, will bucket all values under 1000 into others \n",
    "\n",
    "bucket_classification = list(classification_count[classification_count<1000].index)\n",
    "for classification in bucket_classification: \n",
    "    charity_df[\"CLASSIFICATION\"] = charity_df[\"CLASSIFICATION\"].replace(classification, \"Other\")\n",
    "\n",
    "charity_df[\"CLASSIFICATION\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3     27037\n",
       "T4      1542\n",
       "T6      1216\n",
       "T5      1173\n",
       "T19     1065\n",
       "T8       737\n",
       "T7       725\n",
       "T10      528\n",
       "T9       156\n",
       "T13       66\n",
       "T12       27\n",
       "T2        16\n",
       "T25        3\n",
       "T14        3\n",
       "T29        2\n",
       "T15        2\n",
       "T17        1\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check \"Application Type\" with 17 unique values to see if bucketing is needed \n",
    "application = charity_df[\"APPLICATION_TYPE\"].value_counts()\n",
    "application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PARENT BOOSTER USA INC                                          1260\n",
       "TOPS CLUB INC                                                    765\n",
       "UNITED STATES BOWLING CONGRESS INC                               700\n",
       "WASHINGTON STATE UNIVERSITY                                      492\n",
       "AMATEUR ATHLETIC UNION OF THE UNITED STATES INC                  408\n",
       "                                                                ... \n",
       "GREATER BOISE PUG RESCUE AND PLACEMENT GROUP                       1\n",
       "ARROW SPACE FOR FAMILIES INC                                       1\n",
       "ATLANTA RONALD MCDONALD HOUSE CHARITIES INC                        1\n",
       "OKLAHOMA FAMILY LEGAL ADVOCATES INC                                1\n",
       "MONSIGNOR DOMINIC BLASCO COUNCIL NO 3298 KNIGHTS OF COLUMBUS       1\n",
       "Name: NAME, Length: 19568, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check name column\n",
    "names = charity_df.NAME.value_counts()\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a652c34108>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "names.plot.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other                                                                 25987\n",
       "PARENT BOOSTER USA INC                                                 1260\n",
       "TOPS CLUB INC                                                           765\n",
       "UNITED STATES BOWLING CONGRESS INC                                      700\n",
       "WASHINGTON STATE UNIVERSITY                                             492\n",
       "AMATEUR ATHLETIC UNION OF THE UNITED STATES INC                         408\n",
       "PTA TEXAS CONGRESS                                                      368\n",
       "SOROPTIMIST INTERNATIONAL OF THE AMERICAS INC                           331\n",
       "ALPHA PHI SIGMA                                                         313\n",
       "TOASTMASTERS INTERNATIONAL                                              293\n",
       "MOST WORSHIPFUL STRINGER FREE AND ACCEPTED MASONS                       287\n",
       "LITTLE LEAGUE BASEBALL INC                                              277\n",
       "INTERNATIONAL ASSOCIATION OF LIONS CLUBS                                266\n",
       "MOMS CLUB                                                               210\n",
       "INTERNATIONAL ASSOCIATION OF SHEET METAL AIR RAIL & TRANSPORTATION      206\n",
       "AMERICAN ASSOCIATION OF UNIVERSITY WOMEN                                197\n",
       "FARMERS EDUCATIONAL AND COOPERATIVE UNION OF AMERICA                    166\n",
       "KNIGHTS OF COLUMBUS                                                     158\n",
       "HABITAT FOR HUMANITY INTERNATIONAL INC                                  154\n",
       "TENNESSEE ORDER OF THE EASTERN STAR                                     151\n",
       "VETERANS OF FOREIGN WARS OF THE UNITED STATES AUXILIARY                 144\n",
       "PTA UTAH CONGRESS                                                       140\n",
       "THE UNITED STATES PONY CLUBS INC                                        136\n",
       "CIVITAN INTERNATIONAL                                                   131\n",
       "SIGMA BETA DELTA INC                                                    127\n",
       "MONTANA 4-H FOUNDATION INC                                              107\n",
       "HONOR SOCIETY OF PHI KAPPA PHI                                          107\n",
       "WASHINGTON STATE GRANGE                                                 106\n",
       "UNIVERSITY OF WYOMING                                                   105\n",
       "DEMOLAY INTERNATIONAL                                                   104\n",
       "SERTOMA INC                                                             103\n",
       "Name: NAME, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bucket names \n",
    "bucket_names = list(names[names<100].index)\n",
    "for name in bucket_names: \n",
    "    charity_df[\"NAME\"] = charity_df[\"NAME\"].replace(name, \"Other\")\n",
    "\n",
    "charity_df[\"NAME\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME_ALPHA PHI SIGMA</th>\n",
       "      <th>NAME_AMATEUR ATHLETIC UNION OF THE UNITED STATES INC</th>\n",
       "      <th>NAME_AMERICAN ASSOCIATION OF UNIVERSITY WOMEN</th>\n",
       "      <th>NAME_CIVITAN INTERNATIONAL</th>\n",
       "      <th>NAME_DEMOLAY INTERNATIONAL</th>\n",
       "      <th>NAME_FARMERS EDUCATIONAL AND COOPERATIVE UNION OF AMERICA</th>\n",
       "      <th>NAME_HABITAT FOR HUMANITY INTERNATIONAL INC</th>\n",
       "      <th>NAME_HONOR SOCIETY OF PHI KAPPA PHI</th>\n",
       "      <th>NAME_INTERNATIONAL ASSOCIATION OF LIONS CLUBS</th>\n",
       "      <th>NAME_INTERNATIONAL ASSOCIATION OF SHEET METAL AIR RAIL &amp; TRANSPORTATION</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NAME_ALPHA PHI SIGMA  NAME_AMATEUR ATHLETIC UNION OF THE UNITED STATES INC  \\\n",
       "0                   0.0                                                0.0      \n",
       "1                   0.0                                                0.0      \n",
       "2                   0.0                                                0.0      \n",
       "3                   0.0                                                0.0      \n",
       "4                   0.0                                                0.0      \n",
       "\n",
       "   NAME_AMERICAN ASSOCIATION OF UNIVERSITY WOMEN  NAME_CIVITAN INTERNATIONAL  \\\n",
       "0                                            0.0                         0.0   \n",
       "1                                            0.0                         0.0   \n",
       "2                                            0.0                         0.0   \n",
       "3                                            0.0                         0.0   \n",
       "4                                            0.0                         0.0   \n",
       "\n",
       "   NAME_DEMOLAY INTERNATIONAL  \\\n",
       "0                         0.0   \n",
       "1                         0.0   \n",
       "2                         0.0   \n",
       "3                         0.0   \n",
       "4                         0.0   \n",
       "\n",
       "   NAME_FARMERS EDUCATIONAL AND COOPERATIVE UNION OF AMERICA  \\\n",
       "0                                                0.0           \n",
       "1                                                0.0           \n",
       "2                                                0.0           \n",
       "3                                                0.0           \n",
       "4                                                0.0           \n",
       "\n",
       "   NAME_HABITAT FOR HUMANITY INTERNATIONAL INC  \\\n",
       "0                                          0.0   \n",
       "1                                          0.0   \n",
       "2                                          0.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "\n",
       "   NAME_HONOR SOCIETY OF PHI KAPPA PHI  \\\n",
       "0                                  0.0   \n",
       "1                                  0.0   \n",
       "2                                  0.0   \n",
       "3                                  0.0   \n",
       "4                                  0.0   \n",
       "\n",
       "   NAME_INTERNATIONAL ASSOCIATION OF LIONS CLUBS  \\\n",
       "0                                            0.0   \n",
       "1                                            0.0   \n",
       "2                                            0.0   \n",
       "3                                            0.0   \n",
       "4                                            0.0   \n",
       "\n",
       "   NAME_INTERNATIONAL ASSOCIATION OF SHEET METAL AIR RAIL & TRANSPORTATION  \\\n",
       "0                                                0.0                         \n",
       "1                                                0.0                         \n",
       "2                                                0.0                         \n",
       "3                                                0.0                         \n",
       "4                                                0.0                         \n",
       "\n",
       "   ...  INCOME_AMT_1-9999  INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  \\\n",
       "0  ...                0.0                     0.0                       0.0   \n",
       "1  ...                1.0                     0.0                       0.0   \n",
       "2  ...                0.0                     0.0                       0.0   \n",
       "3  ...                0.0                     1.0                       0.0   \n",
       "4  ...                0.0                     0.0                       1.0   \n",
       "\n",
       "   INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  \\\n",
       "0                 0.0               0.0                     0.0   \n",
       "1                 0.0               0.0                     0.0   \n",
       "2                 0.0               0.0                     0.0   \n",
       "3                 0.0               0.0                     0.0   \n",
       "4                 0.0               0.0                     0.0   \n",
       "\n",
       "   INCOME_AMT_50M+  INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  \\\n",
       "0              0.0                0.0                       1.0   \n",
       "1              0.0                0.0                       1.0   \n",
       "2              0.0                0.0                       1.0   \n",
       "3              0.0                0.0                       1.0   \n",
       "4              0.0                0.0                       1.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_Y  \n",
       "0                       0.0  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode all categorical values \n",
    "enc = OneHotEncoder(sparse = False)\n",
    "encode_df = pd.DataFrame(enc.fit_transform(charity_df[charity_cat]))\n",
    "encode_df.columns = enc.get_feature_names(charity_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the encoded dataframe back into main dataframe, drop the converted columns \n",
    "charity_df = charity_df.merge(encode_df, left_index = True, right_index = True)\n",
    "charity_df = charity_df.drop(charity_cat, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into training and testing data. The target is to predict if the money is used effectively, so \"IS_SUCESSFUL\"\n",
    "#will be the target \n",
    "\n",
    "y = charity_df['IS_SUCCESSFUL'].values\n",
    "X=charity_df.drop(\"IS_SUCCESSFUL\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data \n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random forest predictive accuracy: 0.739\n"
     ]
    }
   ],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\otrin\\.conda\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#build neural network. The total number of rows is over 36,000, with 36 input dimensions.  Because of the large number of \n",
    "#input dimensions, I will build a neural network with 2 layers, first layer with 10 units and second with 8\n",
    "\n",
    "input_features = len(X_train_scaled[0])\n",
    "nodes_layer1 = 10\n",
    "nodes_layer2 = 8\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "nn.add(tf.keras.layers.Dense(units = nodes_layer1, input_dim = input_features, activation = \"relu\"))\n",
    "nn.add(tf.keras.layers.Dense(units = nodes_layer2, activation = \"relu\"))\n",
    "nn.add(tf.keras.layers.Dense(units =1, activation = \"sigmoid\"))\n",
    "nn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.5606 - acc: 0.7065\n",
      "Epoch 2/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.5029 - acc: 0.7510\n",
      "Epoch 3/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4957 - acc: 0.7547\n",
      "Epoch 4/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4928 - acc: 0.7566\n",
      "Epoch 5/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4914 - acc: 0.7569\n",
      "Epoch 6/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4898 - acc: 0.7579\n",
      "Epoch 7/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4894 - acc: 0.7576\n",
      "Epoch 8/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4884 - acc: 0.7593\n",
      "Epoch 9/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4882 - acc: 0.7577\n",
      "Epoch 10/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4875 - acc: 0.7593\n",
      "Epoch 11/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4869 - acc: 0.7595\n",
      "Epoch 12/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4863 - acc: 0.7603\n",
      "Epoch 13/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4860 - acc: 0.7606\n",
      "Epoch 14/50\n",
      "25724/25724 [==============================] - 1s 19us/sample - loss: 0.4859 - acc: 0.7591\n",
      "Epoch 15/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4857 - acc: 0.7598\n",
      "Epoch 16/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4853 - acc: 0.7602\n",
      "Epoch 17/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4852 - acc: 0.7597\n",
      "Epoch 18/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4845 - acc: 0.7612\n",
      "Epoch 19/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4844 - acc: 0.7609\n",
      "Epoch 20/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4841 - acc: 0.7598\n",
      "Epoch 21/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4841 - acc: 0.7610\n",
      "Epoch 22/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4838 - acc: 0.7614\n",
      "Epoch 23/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4838 - acc: 0.7620\n",
      "Epoch 24/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4834 - acc: 0.7617\n",
      "Epoch 25/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4830 - acc: 0.7615\n",
      "Epoch 26/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4828 - acc: 0.7621\n",
      "Epoch 27/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4830 - acc: 0.7624\n",
      "Epoch 28/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4826 - acc: 0.7616\n",
      "Epoch 29/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4826 - acc: 0.7625\n",
      "Epoch 30/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4826 - acc: 0.7616\n",
      "Epoch 31/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4823 - acc: 0.7629\n",
      "Epoch 32/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4822 - acc: 0.7613\n",
      "Epoch 33/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4822 - acc: 0.7618\n",
      "Epoch 34/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4820 - acc: 0.7619\n",
      "Epoch 35/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4818 - acc: 0.7620\n",
      "Epoch 36/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4821 - acc: 0.7613\n",
      "Epoch 37/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4817 - acc: 0.7614\n",
      "Epoch 38/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4818 - acc: 0.7622\n",
      "Epoch 39/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4815 - acc: 0.7624\n",
      "Epoch 40/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4814 - acc: 0.7635\n",
      "Epoch 41/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4813 - acc: 0.7620\n",
      "Epoch 42/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4811 - acc: 0.7627\n",
      "Epoch 43/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4811 - acc: 0.7628\n",
      "Epoch 44/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4813 - acc: 0.7630\n",
      "Epoch 45/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4805 - acc: 0.7631\n",
      "Epoch 46/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4808 - acc: 0.7636\n",
      "Epoch 47/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4806 - acc: 0.7626\n",
      "Epoch 48/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4807 - acc: 0.7631\n",
      "Epoch 49/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4802 - acc: 0.7631\n",
      "Epoch 50/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4804 - acc: 0.7633\n",
      "8575/8575 - 0s - loss: 0.4982 - acc: 0.7520\n",
      "Loss: 0.4982308993812205, Accuracy: 0.7519533634185791\n"
     ]
    }
   ],
   "source": [
    "#train the model \n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs = 50)\n",
    "#evaluate model accuracy\n",
    "model_loss, model_accuracy =nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy at 75%, try optimizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 1: Look at outliers for the ASK_AMT variable to decrease noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000       25398\n",
       "10478          3\n",
       "15583          3\n",
       "6725           3\n",
       "63981          3\n",
       "           ...  \n",
       "772556         1\n",
       "70103          1\n",
       "27096          1\n",
       "25049          1\n",
       "1138700        1\n",
       "Name: ASK_AMT, Length: 8747, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at outliers for the ASK_AMT variable\n",
    "amount = X.ASK_AMT.value_counts()\n",
    "amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8747"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify unique values\n",
    "X.ASK_AMT.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000     25398\n",
       "Other     8901\n",
       "Name: ASK_AMT, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are over 8,000 unique values we'll bucket the values so that those less than 25000 will be grouped together \n",
    "bucket_amount = list(amount[amount<25000].index)\n",
    "for amt in bucket_amount: \n",
    "    X[\"ASK_AMT\"] = X['ASK_AMT'].replace(amt, \"Other\")\n",
    "X.ASK_AMT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASK_AMT_5000</th>\n",
       "      <th>ASK_AMT_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ASK_AMT_5000  ASK_AMT_Other\n",
       "0           1.0            0.0\n",
       "1           0.0            1.0\n",
       "2           1.0            0.0\n",
       "3           0.0            1.0\n",
       "4           0.0            1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"ASK_AMT\"] = X[\"ASK_AMT\"].astype(str)\n",
    "encode_amount = pd.DataFrame(enc.fit_transform(X[\"ASK_AMT\"].values.reshape(-1,1)))\n",
    "encode_amount.columns = enc.get_feature_names(['ASK_AMT'])\n",
    "encode_amount.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.merge(encode_amount, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>NAME_ALPHA PHI SIGMA</th>\n",
       "      <th>NAME_AMATEUR ATHLETIC UNION OF THE UNITED STATES INC</th>\n",
       "      <th>NAME_AMERICAN ASSOCIATION OF UNIVERSITY WOMEN</th>\n",
       "      <th>NAME_CIVITAN INTERNATIONAL</th>\n",
       "      <th>NAME_DEMOLAY INTERNATIONAL</th>\n",
       "      <th>NAME_FARMERS EDUCATIONAL AND COOPERATIVE UNION OF AMERICA</th>\n",
       "      <th>NAME_HABITAT FOR HUMANITY INTERNATIONAL INC</th>\n",
       "      <th>NAME_HONOR SOCIETY OF PHI KAPPA PHI</th>\n",
       "      <th>NAME_INTERNATIONAL ASSOCIATION OF LIONS CLUBS</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "      <th>ASK_AMT_5000</th>\n",
       "      <th>ASK_AMT_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATUS  NAME_ALPHA PHI SIGMA  \\\n",
       "0       1                   0.0   \n",
       "1       1                   0.0   \n",
       "2       1                   0.0   \n",
       "3       1                   0.0   \n",
       "4       1                   0.0   \n",
       "\n",
       "   NAME_AMATEUR ATHLETIC UNION OF THE UNITED STATES INC  \\\n",
       "0                                                0.0      \n",
       "1                                                0.0      \n",
       "2                                                0.0      \n",
       "3                                                0.0      \n",
       "4                                                0.0      \n",
       "\n",
       "   NAME_AMERICAN ASSOCIATION OF UNIVERSITY WOMEN  NAME_CIVITAN INTERNATIONAL  \\\n",
       "0                                            0.0                         0.0   \n",
       "1                                            0.0                         0.0   \n",
       "2                                            0.0                         0.0   \n",
       "3                                            0.0                         0.0   \n",
       "4                                            0.0                         0.0   \n",
       "\n",
       "   NAME_DEMOLAY INTERNATIONAL  \\\n",
       "0                         0.0   \n",
       "1                         0.0   \n",
       "2                         0.0   \n",
       "3                         0.0   \n",
       "4                         0.0   \n",
       "\n",
       "   NAME_FARMERS EDUCATIONAL AND COOPERATIVE UNION OF AMERICA  \\\n",
       "0                                                0.0           \n",
       "1                                                0.0           \n",
       "2                                                0.0           \n",
       "3                                                0.0           \n",
       "4                                                0.0           \n",
       "\n",
       "   NAME_HABITAT FOR HUMANITY INTERNATIONAL INC  \\\n",
       "0                                          0.0   \n",
       "1                                          0.0   \n",
       "2                                          0.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "\n",
       "   NAME_HONOR SOCIETY OF PHI KAPPA PHI  \\\n",
       "0                                  0.0   \n",
       "1                                  0.0   \n",
       "2                                  0.0   \n",
       "3                                  0.0   \n",
       "4                                  0.0   \n",
       "\n",
       "   NAME_INTERNATIONAL ASSOCIATION OF LIONS CLUBS  ...  \\\n",
       "0                                            0.0  ...   \n",
       "1                                            0.0  ...   \n",
       "2                                            0.0  ...   \n",
       "3                                            0.0  ...   \n",
       "4                                            0.0  ...   \n",
       "\n",
       "   INCOME_AMT_100000-499999  INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  \\\n",
       "0                       0.0                 0.0               0.0   \n",
       "1                       0.0                 0.0               0.0   \n",
       "2                       0.0                 0.0               0.0   \n",
       "3                       0.0                 0.0               0.0   \n",
       "4                       1.0                 0.0               0.0   \n",
       "\n",
       "   INCOME_AMT_25000-99999  INCOME_AMT_50M+  INCOME_AMT_5M-10M  \\\n",
       "0                     0.0              0.0                0.0   \n",
       "1                     0.0              0.0                0.0   \n",
       "2                     0.0              0.0                0.0   \n",
       "3                     0.0              0.0                0.0   \n",
       "4                     0.0              0.0                0.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  ASK_AMT_5000  \\\n",
       "0                       1.0                       0.0           1.0   \n",
       "1                       1.0                       0.0           0.0   \n",
       "2                       1.0                       0.0           1.0   \n",
       "3                       1.0                       0.0           0.0   \n",
       "4                       1.0                       0.0           0.0   \n",
       "\n",
       "   ASK_AMT_Other  \n",
       "0            0.0  \n",
       "1            1.0  \n",
       "2            0.0  \n",
       "3            1.0  \n",
       "4            1.0  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop ASK_AMT\n",
    "X=X.drop('ASK_AMT',1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-split, re-scale, and re-build the model with new dataset \n",
    "#Resplit \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale\n",
    "input_features = len(X_train_scaled[0])\n",
    "nodes_layer1 = 10\n",
    "nodes_layer2 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remodel\n",
    "nn = tf.keras.models.Sequential()\n",
    "nn.add(tf.keras.layers.Dense(units = nodes_layer1, input_dim = input_features, activation = \"relu\"))\n",
    "nn.add(tf.keras.layers.Dense(units = nodes_layer2, activation = \"relu\"))\n",
    "nn.add(tf.keras.layers.Dense(units =1, activation = \"sigmoid\"))\n",
    "nn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.5647 - acc: 0.7112\n",
      "Epoch 2/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.5053 - acc: 0.7432\n",
      "Epoch 3/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4972 - acc: 0.7498\n",
      "Epoch 4/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4942 - acc: 0.7521\n",
      "Epoch 5/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4926 - acc: 0.7528\n",
      "Epoch 6/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4916 - acc: 0.7548\n",
      "Epoch 7/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4909 - acc: 0.7566\n",
      "Epoch 8/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4897 - acc: 0.7573\n",
      "Epoch 9/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4886 - acc: 0.7575\n",
      "Epoch 10/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4881 - acc: 0.7575\n",
      "Epoch 11/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4876 - acc: 0.7571\n",
      "Epoch 12/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4873 - acc: 0.7582\n",
      "Epoch 13/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4868 - acc: 0.7582\n",
      "Epoch 14/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4859 - acc: 0.7584\n",
      "Epoch 15/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4855 - acc: 0.7589\n",
      "Epoch 16/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4852 - acc: 0.7589\n",
      "Epoch 17/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4844 - acc: 0.7591\n",
      "Epoch 18/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4844 - acc: 0.7594\n",
      "Epoch 19/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4841 - acc: 0.7597\n",
      "Epoch 20/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4835 - acc: 0.7593\n",
      "Epoch 21/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4835 - acc: 0.7583\n",
      "Epoch 22/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4834 - acc: 0.7603\n",
      "Epoch 23/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4834 - acc: 0.7603\n",
      "Epoch 24/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4830 - acc: 0.7600\n",
      "Epoch 25/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4828 - acc: 0.7609\n",
      "Epoch 26/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4823 - acc: 0.7608\n",
      "Epoch 27/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4820 - acc: 0.7612\n",
      "Epoch 28/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4822 - acc: 0.7605\n",
      "Epoch 29/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4818 - acc: 0.7605\n",
      "Epoch 30/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4814 - acc: 0.7607\n",
      "Epoch 31/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4812 - acc: 0.7613\n",
      "Epoch 32/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4815 - acc: 0.7622\n",
      "Epoch 33/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4812 - acc: 0.7613\n",
      "Epoch 34/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4811 - acc: 0.7626\n",
      "Epoch 35/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4811 - acc: 0.7629\n",
      "Epoch 36/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4808 - acc: 0.7622\n",
      "Epoch 37/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4807 - acc: 0.7626\n",
      "Epoch 38/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4802 - acc: 0.7616\n",
      "Epoch 39/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4804 - acc: 0.7634\n",
      "Epoch 40/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4805 - acc: 0.7612\n",
      "Epoch 41/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4798 - acc: 0.7625\n",
      "Epoch 42/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4800 - acc: 0.7633\n",
      "Epoch 43/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4801 - acc: 0.7633\n",
      "Epoch 44/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4798 - acc: 0.7638\n",
      "Epoch 45/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4795 - acc: 0.7631\n",
      "Epoch 46/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4796 - acc: 0.7637\n",
      "Epoch 47/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4797 - acc: 0.7635\n",
      "Epoch 48/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4790 - acc: 0.7634\n",
      "Epoch 49/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4792 - acc: 0.7648\n",
      "Epoch 50/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4791 - acc: 0.7642\n",
      "8575/8575 - 0s - loss: 0.4971 - acc: 0.7507\n",
      "Loss: 0.49710106219216615, Accuracy: 0.7506705522537231\n"
     ]
    }
   ],
   "source": [
    "# Refit\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs = 50)\n",
    "#evaluate model accuracy\n",
    "model_loss, model_accuracy =nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy minimally lowered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 2: Try adding more neurons to first layer (10 -> 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25724/25724 [==============================] - 1s 23us/sample - loss: 0.5419 - acc: 0.7252\n",
      "Epoch 2/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4992 - acc: 0.7495\n",
      "Epoch 3/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4933 - acc: 0.7544\n",
      "Epoch 4/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4904 - acc: 0.7563\n",
      "Epoch 5/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4885 - acc: 0.7576\n",
      "Epoch 6/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4872 - acc: 0.7570\n",
      "Epoch 7/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4862 - acc: 0.7580\n",
      "Epoch 8/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4853 - acc: 0.7584\n",
      "Epoch 9/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4842 - acc: 0.7588\n",
      "Epoch 10/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4840 - acc: 0.7597\n",
      "Epoch 11/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4833 - acc: 0.7594\n",
      "Epoch 12/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4829 - acc: 0.7609\n",
      "Epoch 13/50\n",
      "25724/25724 [==============================] - 1s 19us/sample - loss: 0.4825 - acc: 0.7604\n",
      "Epoch 14/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4820 - acc: 0.7601\n",
      "Epoch 15/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4821 - acc: 0.7619\n",
      "Epoch 16/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4813 - acc: 0.7621\n",
      "Epoch 17/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4812 - acc: 0.7623\n",
      "Epoch 18/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4807 - acc: 0.7632\n",
      "Epoch 19/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4812 - acc: 0.7614\n",
      "Epoch 20/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4805 - acc: 0.7639\n",
      "Epoch 21/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4808 - acc: 0.7623\n",
      "Epoch 22/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4798 - acc: 0.7631\n",
      "Epoch 23/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4800 - acc: 0.7633\n",
      "Epoch 24/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4794 - acc: 0.7632\n",
      "Epoch 25/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4795 - acc: 0.7617\n",
      "Epoch 26/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4792 - acc: 0.7643\n",
      "Epoch 27/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4793 - acc: 0.7637\n",
      "Epoch 28/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4791 - acc: 0.7641\n",
      "Epoch 29/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4790 - acc: 0.7639\n",
      "Epoch 30/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4785 - acc: 0.7648\n",
      "Epoch 31/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4786 - acc: 0.7645\n",
      "Epoch 32/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4788 - acc: 0.7644\n",
      "Epoch 33/50\n",
      "25724/25724 [==============================] - 0s 19us/sample - loss: 0.4783 - acc: 0.7646\n",
      "Epoch 34/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4784 - acc: 0.7634\n",
      "Epoch 35/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4780 - acc: 0.7639\n",
      "Epoch 36/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4779 - acc: 0.7653\n",
      "Epoch 37/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4777 - acc: 0.7654\n",
      "Epoch 38/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4780 - acc: 0.7642\n",
      "Epoch 39/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4775 - acc: 0.7637\n",
      "Epoch 40/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4772 - acc: 0.7639\n",
      "Epoch 41/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4772 - acc: 0.7650\n",
      "Epoch 42/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4772 - acc: 0.7642\n",
      "Epoch 43/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4771 - acc: 0.7634\n",
      "Epoch 44/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4765 - acc: 0.7659\n",
      "Epoch 45/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4768 - acc: 0.7661\n",
      "Epoch 46/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4767 - acc: 0.7646\n",
      "Epoch 47/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4764 - acc: 0.7636\n",
      "Epoch 48/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4766 - acc: 0.7641\n",
      "Epoch 49/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4763 - acc: 0.7642\n",
      "Epoch 50/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4765 - acc: 0.7647\n",
      "8575/8575 - 0s - loss: 0.4988 - acc: 0.7537\n",
      "Loss: 0.4987882330431535, Accuracy: 0.7537026405334473\n"
     ]
    }
   ],
   "source": [
    "new_nodes_layer1 = 20\n",
    "nodes_layer2 = 8\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "nn.add(tf.keras.layers.Dense(units = new_nodes_layer1, input_dim = input_features, activation = \"relu\"))\n",
    "nn.add(tf.keras.layers.Dense(units = nodes_layer2, activation = \"relu\"))\n",
    "nn.add(tf.keras.layers.Dense(units =1, activation = \"sigmoid\"))\n",
    "nn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs = 50)\n",
    "#evaluate model accuracy\n",
    "model_loss, model_accuracy =nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 3: Try adding more neurons to second layer (8 -> 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25724/25724 [==============================] - 1s 23us/sample - loss: 0.5294 - acc: 0.7305\n",
      "Epoch 2/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4964 - acc: 0.7548\n",
      "Epoch 3/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4923 - acc: 0.7553\n",
      "Epoch 4/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4896 - acc: 0.7566\n",
      "Epoch 5/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4884 - acc: 0.7573\n",
      "Epoch 6/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4868 - acc: 0.7578\n",
      "Epoch 7/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4859 - acc: 0.7589\n",
      "Epoch 8/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4852 - acc: 0.7594\n",
      "Epoch 9/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4846 - acc: 0.7600\n",
      "Epoch 10/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4841 - acc: 0.7603\n",
      "Epoch 11/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4838 - acc: 0.7594\n",
      "Epoch 12/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4835 - acc: 0.7609\n",
      "Epoch 13/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4830 - acc: 0.7612\n",
      "Epoch 14/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4826 - acc: 0.7611\n",
      "Epoch 15/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4825 - acc: 0.7612\n",
      "Epoch 16/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4813 - acc: 0.7635\n",
      "Epoch 17/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4815 - acc: 0.7624\n",
      "Epoch 18/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4814 - acc: 0.7617\n",
      "Epoch 19/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4806 - acc: 0.7620\n",
      "Epoch 20/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4805 - acc: 0.7641\n",
      "Epoch 21/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4803 - acc: 0.7617\n",
      "Epoch 22/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4801 - acc: 0.7631\n",
      "Epoch 23/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4800 - acc: 0.7635\n",
      "Epoch 24/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4798 - acc: 0.7624\n",
      "Epoch 25/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4795 - acc: 0.7636\n",
      "Epoch 26/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4798 - acc: 0.7626\n",
      "Epoch 27/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4795 - acc: 0.7635\n",
      "Epoch 28/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4789 - acc: 0.7629\n",
      "Epoch 29/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4787 - acc: 0.7638\n",
      "Epoch 30/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4782 - acc: 0.7643\n",
      "Epoch 31/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4784 - acc: 0.7635\n",
      "Epoch 32/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4785 - acc: 0.7640\n",
      "Epoch 33/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4780 - acc: 0.7635\n",
      "Epoch 34/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4781 - acc: 0.7645\n",
      "Epoch 35/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4777 - acc: 0.7637\n",
      "Epoch 36/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4775 - acc: 0.7652\n",
      "Epoch 37/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4773 - acc: 0.7646\n",
      "Epoch 38/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4774 - acc: 0.7644\n",
      "Epoch 39/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4774 - acc: 0.7645\n",
      "Epoch 40/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4774 - acc: 0.7646\n",
      "Epoch 41/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4772 - acc: 0.7639\n",
      "Epoch 42/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4770 - acc: 0.7642\n",
      "Epoch 43/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4771 - acc: 0.7647\n",
      "Epoch 44/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4768 - acc: 0.7640\n",
      "Epoch 45/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4768 - acc: 0.7645\n",
      "Epoch 46/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4767 - acc: 0.7648\n",
      "Epoch 47/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4766 - acc: 0.7635\n",
      "Epoch 48/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4766 - acc: 0.7637\n",
      "Epoch 49/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4764 - acc: 0.7643\n",
      "Epoch 50/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4763 - acc: 0.7635\n",
      "8575/8575 - 0s - loss: 0.4955 - acc: 0.7520\n",
      "Loss: 0.49553741139851226, Accuracy: 0.7519533634185791\n"
     ]
    }
   ],
   "source": [
    "new_nodes_layer1 = 20\n",
    "new_nodes_layer2 = 12\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "nn.add(tf.keras.layers.Dense(units = new_nodes_layer1, input_dim = input_features, activation = \"relu\"))\n",
    "nn.add(tf.keras.layers.Dense(units = new_nodes_layer2, activation = \"relu\"))\n",
    "nn.add(tf.keras.layers.Dense(units =1, activation = \"sigmoid\"))\n",
    "nn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs = 50)\n",
    "#evaluate model accuracy\n",
    "model_loss, model_accuracy =nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 4: Try a different activation method with 12 layer2 nodes (Relu to LeakyRelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Maintain the previously tried node settings in Optimization 2 with different activation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25724/25724 [==============================] - 1s 23us/sample - loss: 0.5292 - acc: 0.7295\n",
      "Epoch 2/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4984 - acc: 0.7464\n",
      "Epoch 3/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4949 - acc: 0.7518\n",
      "Epoch 4/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4932 - acc: 0.7541\n",
      "Epoch 5/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4918 - acc: 0.7557\n",
      "Epoch 6/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4905 - acc: 0.7577\n",
      "Epoch 7/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4890 - acc: 0.7588\n",
      "Epoch 8/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4887 - acc: 0.7573\n",
      "Epoch 9/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4880 - acc: 0.7588\n",
      "Epoch 10/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4869 - acc: 0.7601\n",
      "Epoch 11/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4866 - acc: 0.7580\n",
      "Epoch 12/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4865 - acc: 0.7595\n",
      "Epoch 13/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4859 - acc: 0.7607\n",
      "Epoch 14/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4850 - acc: 0.7594\n",
      "Epoch 15/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4853 - acc: 0.7608\n",
      "Epoch 16/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4850 - acc: 0.7604\n",
      "Epoch 17/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4846 - acc: 0.7602\n",
      "Epoch 18/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4841 - acc: 0.7608\n",
      "Epoch 19/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4841 - acc: 0.7612\n",
      "Epoch 20/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4838 - acc: 0.7612\n",
      "Epoch 21/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4836 - acc: 0.7596\n",
      "Epoch 22/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4832 - acc: 0.7622\n",
      "Epoch 23/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4832 - acc: 0.7611\n",
      "Epoch 24/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4829 - acc: 0.7609\n",
      "Epoch 25/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4830 - acc: 0.7617\n",
      "Epoch 26/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4828 - acc: 0.7615\n",
      "Epoch 27/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4823 - acc: 0.7633\n",
      "Epoch 28/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4825 - acc: 0.7607\n",
      "Epoch 29/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4820 - acc: 0.7637\n",
      "Epoch 30/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4823 - acc: 0.7615\n",
      "Epoch 31/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4816 - acc: 0.7625\n",
      "Epoch 32/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4819 - acc: 0.7619\n",
      "Epoch 33/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4815 - acc: 0.7632\n",
      "Epoch 34/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4817 - acc: 0.7617\n",
      "Epoch 35/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4812 - acc: 0.7636\n",
      "Epoch 36/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4814 - acc: 0.7629\n",
      "Epoch 37/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4810 - acc: 0.7619\n",
      "Epoch 38/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4810 - acc: 0.7622\n",
      "Epoch 39/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4809 - acc: 0.7626\n",
      "Epoch 40/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4806 - acc: 0.7636\n",
      "Epoch 41/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4809 - acc: 0.7635\n",
      "Epoch 42/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4807 - acc: 0.7634\n",
      "Epoch 43/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4802 - acc: 0.7631\n",
      "Epoch 44/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4799 - acc: 0.7639\n",
      "Epoch 45/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4801 - acc: 0.7631\n",
      "Epoch 46/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4802 - acc: 0.7624\n",
      "Epoch 47/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4800 - acc: 0.7643\n",
      "Epoch 48/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4799 - acc: 0.7626\n",
      "Epoch 49/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4801 - acc: 0.7641\n",
      "Epoch 50/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4798 - acc: 0.7642\n",
      "8575/8575 - 0s - loss: 0.4973 - acc: 0.7544\n",
      "Loss: 0.4972598652033347, Accuracy: 0.7544023394584656\n"
     ]
    }
   ],
   "source": [
    "new_nodes_layer1 = 20\n",
    "nodes_layer2 = 12\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "nn.add(tf.keras.layers.Dense(units = new_nodes_layer1, input_dim = input_features, activation = tf.keras.layers.LeakyReLU(alpha=0.3)))\n",
    "nn.add(tf.keras.layers.Dense(units = nodes_layer2, activation = tf.keras.layers.LeakyReLU(alpha=0.3)))\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units =1, activation = \"sigmoid\"))\n",
    "nn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs = 50)\n",
    "#evaluate model accuracy\n",
    "model_loss, model_accuracy =nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Slight improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Go back to original node ratio of 20 : 8 with different activation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25724/25724 [==============================] - 1s 23us/sample - loss: 0.5322 - acc: 0.7262\n",
      "Epoch 2/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4968 - acc: 0.7512\n",
      "Epoch 3/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4934 - acc: 0.7563\n",
      "Epoch 4/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4910 - acc: 0.7563\n",
      "Epoch 5/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4903 - acc: 0.7575\n",
      "Epoch 6/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4897 - acc: 0.7566\n",
      "Epoch 7/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4886 - acc: 0.7584\n",
      "Epoch 8/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4881 - acc: 0.7588\n",
      "Epoch 9/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4876 - acc: 0.7580\n",
      "Epoch 10/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4874 - acc: 0.7594\n",
      "Epoch 11/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4867 - acc: 0.7578\n",
      "Epoch 12/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4864 - acc: 0.7583\n",
      "Epoch 13/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4862 - acc: 0.7600\n",
      "Epoch 14/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4862 - acc: 0.7598\n",
      "Epoch 15/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4856 - acc: 0.7597\n",
      "Epoch 16/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4852 - acc: 0.7600\n",
      "Epoch 17/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4850 - acc: 0.7597\n",
      "Epoch 18/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4848 - acc: 0.7613\n",
      "Epoch 19/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4844 - acc: 0.7596\n",
      "Epoch 20/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4845 - acc: 0.7590\n",
      "Epoch 21/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4841 - acc: 0.7603\n",
      "Epoch 22/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4839 - acc: 0.7612\n",
      "Epoch 23/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4836 - acc: 0.7596\n",
      "Epoch 24/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4839 - acc: 0.7602\n",
      "Epoch 25/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4833 - acc: 0.7612\n",
      "Epoch 26/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4833 - acc: 0.7609\n",
      "Epoch 27/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4832 - acc: 0.7614\n",
      "Epoch 28/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4829 - acc: 0.7605\n",
      "Epoch 29/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4829 - acc: 0.7608\n",
      "Epoch 30/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4827 - acc: 0.7606\n",
      "Epoch 31/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4824 - acc: 0.7613\n",
      "Epoch 32/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4825 - acc: 0.7615\n",
      "Epoch 33/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4823 - acc: 0.7610\n",
      "Epoch 34/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4823 - acc: 0.7620\n",
      "Epoch 35/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4820 - acc: 0.7622\n",
      "Epoch 36/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4817 - acc: 0.7619\n",
      "Epoch 37/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4815 - acc: 0.7623\n",
      "Epoch 38/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4815 - acc: 0.7613\n",
      "Epoch 39/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4816 - acc: 0.7617\n",
      "Epoch 40/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4812 - acc: 0.7614\n",
      "Epoch 41/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4812 - acc: 0.7620\n",
      "Epoch 42/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4811 - acc: 0.7623\n",
      "Epoch 43/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4813 - acc: 0.7623\n",
      "Epoch 44/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4810 - acc: 0.7628\n",
      "Epoch 45/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4809 - acc: 0.7619\n",
      "Epoch 46/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4807 - acc: 0.7622\n",
      "Epoch 47/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4806 - acc: 0.7629\n",
      "Epoch 48/50\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4803 - acc: 0.7624\n",
      "Epoch 49/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4805 - acc: 0.7633\n",
      "Epoch 50/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4806 - acc: 0.7635\n",
      "8575/8575 - 0s - loss: 0.4964 - acc: 0.7551\n",
      "Loss: 0.49642045741873647, Accuracy: 0.7551020383834839\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Go back to original node ratio of 20 : 8 with different activation method\n",
    "new_nodes_layer1 = 20\n",
    "nodes_layer2 = 8\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "nn.add(tf.keras.layers.Dense(units = new_nodes_layer1, input_dim = input_features, activation = tf.keras.layers.LeakyReLU(alpha=0.3)))\n",
    "nn.add(tf.keras.layers.Dense(units = nodes_layer2, activation = tf.keras.layers.LeakyReLU(alpha=0.3)))\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units =1, activation = \"sigmoid\"))\n",
    "nn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs = 50)\n",
    "#evaluate model accuracy\n",
    "model_loss, model_accuracy =nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy increased compared to 4.1 optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Loss and Accuracy of the Most Accurate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training the history of the loss and accuracy history.\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the plot visualizing that as the loss is reduced the accuracy increases over each epoch.\n",
    "loss_test = history_df['loss']\n",
    "accur_test = history_df['acc']\n",
    "epochs = range(1,200)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(loss_test, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis.\n",
    "\n",
    "color = 'tab:green'\n",
    "ax2.set_ylabel('Accuracy', color=color)\n",
    "ax2.plot(accur_test, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # Ensuring the right y-label is not slightly clipped.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 4: Increase Number of Epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25724/25724 [==============================] - 1s 25us/sample - loss: 0.5381 - acc: 0.7296\n",
      "Epoch 2/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4997 - acc: 0.7491\n",
      "Epoch 3/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4951 - acc: 0.7538\n",
      "Epoch 4/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4940 - acc: 0.7554\n",
      "Epoch 5/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4915 - acc: 0.7570\n",
      "Epoch 6/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4906 - acc: 0.7564\n",
      "Epoch 7/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4899 - acc: 0.7566\n",
      "Epoch 8/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4885 - acc: 0.7572\n",
      "Epoch 9/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4883 - acc: 0.7580\n",
      "Epoch 10/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4878 - acc: 0.7579\n",
      "Epoch 11/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4872 - acc: 0.7592\n",
      "Epoch 12/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4872 - acc: 0.7580\n",
      "Epoch 13/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4867 - acc: 0.7585\n",
      "Epoch 14/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4862 - acc: 0.7585\n",
      "Epoch 15/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4858 - acc: 0.7598\n",
      "Epoch 16/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4851 - acc: 0.7586\n",
      "Epoch 17/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4854 - acc: 0.7598\n",
      "Epoch 18/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4850 - acc: 0.7594\n",
      "Epoch 19/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4849 - acc: 0.7587\n",
      "Epoch 20/100\n",
      "25724/25724 [==============================] - ETA: 0s - loss: 0.4833 - acc: 0.760 - 1s 21us/sample - loss: 0.4842 - acc: 0.7597\n",
      "Epoch 21/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4842 - acc: 0.7593\n",
      "Epoch 22/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4841 - acc: 0.7586\n",
      "Epoch 23/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4839 - acc: 0.7579\n",
      "Epoch 24/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4835 - acc: 0.7596\n",
      "Epoch 25/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4834 - acc: 0.7593\n",
      "Epoch 26/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4830 - acc: 0.7605\n",
      "Epoch 27/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4834 - acc: 0.7595\n",
      "Epoch 28/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4830 - acc: 0.7600\n",
      "Epoch 29/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4825 - acc: 0.7608\n",
      "Epoch 30/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4823 - acc: 0.7592\n",
      "Epoch 31/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4827 - acc: 0.7605\n",
      "Epoch 32/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4819 - acc: 0.7602\n",
      "Epoch 33/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4821 - acc: 0.7599\n",
      "Epoch 34/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4820 - acc: 0.7612\n",
      "Epoch 35/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4820 - acc: 0.7612\n",
      "Epoch 36/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4815 - acc: 0.7621\n",
      "Epoch 37/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4816 - acc: 0.7604\n",
      "Epoch 38/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4818 - acc: 0.7601\n",
      "Epoch 39/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4816 - acc: 0.7610\n",
      "Epoch 40/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4812 - acc: 0.7611\n",
      "Epoch 41/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4812 - acc: 0.7610\n",
      "Epoch 42/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4813 - acc: 0.7612\n",
      "Epoch 43/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4810 - acc: 0.7615\n",
      "Epoch 44/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4811 - acc: 0.7610\n",
      "Epoch 45/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4810 - acc: 0.7605\n",
      "Epoch 46/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4809 - acc: 0.7614\n",
      "Epoch 47/100\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4804 - acc: 0.7613\n",
      "Epoch 48/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4805 - acc: 0.7617\n",
      "Epoch 49/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4806 - acc: 0.7603\n",
      "Epoch 50/100\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4805 - acc: 0.7619\n",
      "Epoch 51/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4802 - acc: 0.7621\n",
      "Epoch 52/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4801 - acc: 0.7623\n",
      "Epoch 53/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4802 - acc: 0.7622\n",
      "Epoch 54/100\n",
      "25724/25724 [==============================] - ETA: 0s - loss: 0.4794 - acc: 0.762 - 1s 21us/sample - loss: 0.4801 - acc: 0.7625\n",
      "Epoch 55/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4801 - acc: 0.7618\n",
      "Epoch 56/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4796 - acc: 0.7635\n",
      "Epoch 57/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4797 - acc: 0.7622\n",
      "Epoch 58/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4795 - acc: 0.7625\n",
      "Epoch 59/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4792 - acc: 0.7636\n",
      "Epoch 60/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4797 - acc: 0.7631\n",
      "Epoch 61/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4790 - acc: 0.7640\n",
      "Epoch 62/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4792 - acc: 0.7636\n",
      "Epoch 63/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4792 - acc: 0.7636\n",
      "Epoch 64/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4792 - acc: 0.7626\n",
      "Epoch 65/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4790 - acc: 0.7629\n",
      "Epoch 66/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4786 - acc: 0.7631\n",
      "Epoch 67/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4789 - acc: 0.7635\n",
      "Epoch 68/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4789 - acc: 0.7633\n",
      "Epoch 69/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4786 - acc: 0.7626\n",
      "Epoch 70/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4789 - acc: 0.7645\n",
      "Epoch 71/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4789 - acc: 0.7629\n",
      "Epoch 72/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4784 - acc: 0.7633\n",
      "Epoch 73/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4786 - acc: 0.7639\n",
      "Epoch 74/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4784 - acc: 0.7637\n",
      "Epoch 75/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4785 - acc: 0.7634\n",
      "Epoch 76/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4782 - acc: 0.7647\n",
      "Epoch 77/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4782 - acc: 0.7626\n",
      "Epoch 78/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4784 - acc: 0.7629\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4786 - acc: 0.7649\n",
      "Epoch 80/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4783 - acc: 0.7640\n",
      "Epoch 81/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4781 - acc: 0.7650\n",
      "Epoch 82/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4783 - acc: 0.7622\n",
      "Epoch 83/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4782 - acc: 0.7646\n",
      "Epoch 84/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4779 - acc: 0.7635\n",
      "Epoch 85/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4780 - acc: 0.7644\n",
      "Epoch 86/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4780 - acc: 0.7647\n",
      "Epoch 87/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4775 - acc: 0.7652\n",
      "Epoch 88/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4779 - acc: 0.7629\n",
      "Epoch 89/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4776 - acc: 0.7646\n",
      "Epoch 90/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4781 - acc: 0.7640\n",
      "Epoch 91/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4775 - acc: 0.7650\n",
      "Epoch 92/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4777 - acc: 0.7644\n",
      "Epoch 93/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4772 - acc: 0.7643\n",
      "Epoch 94/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4772 - acc: 0.7643\n",
      "Epoch 95/100\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4774 - acc: 0.7641\n",
      "Epoch 96/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4774 - acc: 0.7629\n",
      "Epoch 97/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4772 - acc: 0.7649\n",
      "Epoch 98/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4773 - acc: 0.7644\n",
      "Epoch 99/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4773 - acc: 0.7649\n",
      "Epoch 100/100\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4771 - acc: 0.7633\n",
      "8575/8575 - 0s - loss: 0.4988 - acc: 0.7513\n",
      "Loss: 0.49875359777111355, Accuracy: 0.7512536644935608\n"
     ]
    }
   ],
   "source": [
    "# Double Training epochs (50 to 100)\n",
    "new_nodes_layer1 = 20\n",
    "nodes_layer2 = 8\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "nn.add(tf.keras.layers.Dense(units = new_nodes_layer1, input_dim = input_features, activation = tf.keras.layers.LeakyReLU(alpha=0.3)))\n",
    "nn.add(tf.keras.layers.Dense(units = nodes_layer2, activation = tf.keras.layers.LeakyReLU(alpha=0.3)))\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units =1, activation = \"sigmoid\"))\n",
    "nn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs = 100)\n",
    "#evaluate model accuracy\n",
    "model_loss, model_accuracy =nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doubling the training epochs lowerred the accuracy (not a good idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "25724/25724 [==============================] - 1s 24us/sample - loss: 0.5262 - acc: 0.7338\n",
      "Epoch 2/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4976 - acc: 0.7550\n",
      "Epoch 3/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4942 - acc: 0.7565\n",
      "Epoch 4/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4928 - acc: 0.7572\n",
      "Epoch 5/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4916 - acc: 0.7570\n",
      "Epoch 6/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4907 - acc: 0.7571\n",
      "Epoch 7/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4897 - acc: 0.7579\n",
      "Epoch 8/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4892 - acc: 0.7569\n",
      "Epoch 9/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4889 - acc: 0.7566\n",
      "Epoch 10/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4884 - acc: 0.7573\n",
      "Epoch 11/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4882 - acc: 0.7561\n",
      "Epoch 12/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4875 - acc: 0.7571\n",
      "Epoch 13/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4869 - acc: 0.7582\n",
      "Epoch 14/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4867 - acc: 0.7572\n",
      "Epoch 15/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4867 - acc: 0.7594\n",
      "Epoch 16/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4863 - acc: 0.7582\n",
      "Epoch 17/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4860 - acc: 0.7589\n",
      "Epoch 18/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4856 - acc: 0.7592\n",
      "Epoch 19/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4853 - acc: 0.7592\n",
      "Epoch 20/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4852 - acc: 0.7586\n",
      "Epoch 21/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4848 - acc: 0.7586\n",
      "Epoch 22/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4849 - acc: 0.7603\n",
      "Epoch 23/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4848 - acc: 0.7594\n",
      "Epoch 24/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4842 - acc: 0.7585\n",
      "Epoch 25/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4845 - acc: 0.7582\n",
      "Epoch 26/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4840 - acc: 0.7607\n",
      "Epoch 27/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4840 - acc: 0.7600\n",
      "Epoch 28/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4839 - acc: 0.7589\n",
      "Epoch 29/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4834 - acc: 0.7596\n",
      "Epoch 30/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4836 - acc: 0.7600\n",
      "Epoch 31/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4835 - acc: 0.7593\n",
      "Epoch 32/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4833 - acc: 0.7592\n",
      "Epoch 33/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4831 - acc: 0.7610\n",
      "Epoch 34/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4829 - acc: 0.7613\n",
      "Epoch 35/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4831 - acc: 0.7604\n",
      "Epoch 36/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4826 - acc: 0.7609\n",
      "Epoch 37/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4828 - acc: 0.7596\n",
      "Epoch 38/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4825 - acc: 0.7603\n",
      "Epoch 39/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4827 - acc: 0.7592\n",
      "Epoch 40/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4820 - acc: 0.7608\n",
      "Epoch 41/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4819 - acc: 0.7622\n",
      "Epoch 42/75\n",
      "25724/25724 [==============================] - 1s 23us/sample - loss: 0.4818 - acc: 0.7619\n",
      "Epoch 43/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4818 - acc: 0.7614\n",
      "Epoch 44/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4819 - acc: 0.7611\n",
      "Epoch 45/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4818 - acc: 0.7612\n",
      "Epoch 46/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4813 - acc: 0.7612\n",
      "Epoch 47/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4817 - acc: 0.7602\n",
      "Epoch 48/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4814 - acc: 0.7617\n",
      "Epoch 49/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4815 - acc: 0.7610\n",
      "Epoch 50/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4812 - acc: 0.7621\n",
      "Epoch 51/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4811 - acc: 0.7606\n",
      "Epoch 52/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4812 - acc: 0.7617\n",
      "Epoch 53/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4810 - acc: 0.7623\n",
      "Epoch 54/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4810 - acc: 0.7617\n",
      "Epoch 55/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4806 - acc: 0.7625\n",
      "Epoch 56/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4804 - acc: 0.7620\n",
      "Epoch 57/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4807 - acc: 0.7619\n",
      "Epoch 58/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4808 - acc: 0.7631\n",
      "Epoch 59/75\n",
      "25724/25724 [==============================] - 1s 20us/sample - loss: 0.4806 - acc: 0.7626\n",
      "Epoch 60/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4803 - acc: 0.7640\n",
      "Epoch 61/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4802 - acc: 0.7634\n",
      "Epoch 62/75\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4800 - acc: 0.7612\n",
      "Epoch 63/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4802 - acc: 0.7621\n",
      "Epoch 64/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4801 - acc: 0.7607\n",
      "Epoch 65/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4799 - acc: 0.7628\n",
      "Epoch 66/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4796 - acc: 0.7634\n",
      "Epoch 67/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4799 - acc: 0.7635\n",
      "Epoch 68/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4798 - acc: 0.7631\n",
      "Epoch 69/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4796 - acc: 0.7633\n",
      "Epoch 70/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4794 - acc: 0.7625\n",
      "Epoch 71/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4797 - acc: 0.7630\n",
      "Epoch 72/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4797 - acc: 0.7634\n",
      "Epoch 73/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4794 - acc: 0.7625\n",
      "Epoch 74/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4793 - acc: 0.7626\n",
      "Epoch 75/75\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4796 - acc: 0.7636\n",
      "8575/8575 - 0s - loss: 0.4948 - acc: 0.7545\n",
      "Loss: 0.494797987517393, Accuracy: 0.7545189261436462\n"
     ]
    }
   ],
   "source": [
    "# Increase the Epoch from 50 to 75\n",
    "new_nodes_layer1 = 20\n",
    "nodes_layer2 = 8\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "nn.add(tf.keras.layers.Dense(units = new_nodes_layer1, input_dim = input_features, activation = tf.keras.layers.LeakyReLU(alpha=0.3)))\n",
    "nn.add(tf.keras.layers.Dense(units = nodes_layer2, activation = tf.keras.layers.LeakyReLU(alpha=0.3)))\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units =1, activation = \"sigmoid\"))\n",
    "nn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs = 75)\n",
    "#evaluate model accuracy\n",
    "model_loss, model_accuracy =nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing by only 25 epochs got it back up to 75% accuracy but still less than optimization 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 4: Decrease alpha to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25724/25724 [==============================] - 1s 25us/sample - loss: 0.5440 - acc: 0.7284\n",
      "Epoch 2/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4985 - acc: 0.7551\n",
      "Epoch 3/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4931 - acc: 0.7553\n",
      "Epoch 4/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4915 - acc: 0.7573\n",
      "Epoch 5/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4899 - acc: 0.7575\n",
      "Epoch 6/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4888 - acc: 0.7581\n",
      "Epoch 7/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4879 - acc: 0.7581\n",
      "Epoch 8/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4877 - acc: 0.7588\n",
      "Epoch 9/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4871 - acc: 0.7582\n",
      "Epoch 10/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4866 - acc: 0.7578\n",
      "Epoch 11/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4859 - acc: 0.7584\n",
      "Epoch 12/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4851 - acc: 0.7590\n",
      "Epoch 13/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4849 - acc: 0.7584\n",
      "Epoch 14/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4847 - acc: 0.7589\n",
      "Epoch 15/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4843 - acc: 0.7602\n",
      "Epoch 16/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4836 - acc: 0.7607\n",
      "Epoch 17/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4833 - acc: 0.7600\n",
      "Epoch 18/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4830 - acc: 0.7604\n",
      "Epoch 19/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4829 - acc: 0.7606\n",
      "Epoch 20/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4823 - acc: 0.7617\n",
      "Epoch 21/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4819 - acc: 0.7609\n",
      "Epoch 22/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4820 - acc: 0.7607\n",
      "Epoch 23/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4816 - acc: 0.7613\n",
      "Epoch 24/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4818 - acc: 0.7626\n",
      "Epoch 25/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4814 - acc: 0.7618\n",
      "Epoch 26/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4811 - acc: 0.7629\n",
      "Epoch 27/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4813 - acc: 0.7610\n",
      "Epoch 28/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4807 - acc: 0.7619\n",
      "Epoch 29/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4811 - acc: 0.7611\n",
      "Epoch 30/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4803 - acc: 0.7616\n",
      "Epoch 31/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4799 - acc: 0.7622\n",
      "Epoch 32/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4807 - acc: 0.7621\n",
      "Epoch 33/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4805 - acc: 0.7619\n",
      "Epoch 34/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4802 - acc: 0.7619\n",
      "Epoch 35/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4798 - acc: 0.7624\n",
      "Epoch 36/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4797 - acc: 0.7625\n",
      "Epoch 37/50\n",
      "25724/25724 [==============================] - 1s 21us/sample - loss: 0.4794 - acc: 0.7619\n",
      "Epoch 38/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4796 - acc: 0.7631\n",
      "Epoch 39/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4792 - acc: 0.7635\n",
      "Epoch 40/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4790 - acc: 0.7640\n",
      "Epoch 41/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4794 - acc: 0.7630\n",
      "Epoch 42/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4791 - acc: 0.7624\n",
      "Epoch 43/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4787 - acc: 0.7640\n",
      "Epoch 44/50\n",
      "25724/25724 [==============================] - 1s 23us/sample - loss: 0.4787 - acc: 0.7635\n",
      "Epoch 45/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4792 - acc: 0.7634\n",
      "Epoch 46/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4786 - acc: 0.7634\n",
      "Epoch 47/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4782 - acc: 0.7640\n",
      "Epoch 48/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4789 - acc: 0.7633\n",
      "Epoch 49/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4780 - acc: 0.7637\n",
      "Epoch 50/50\n",
      "25724/25724 [==============================] - 1s 22us/sample - loss: 0.4783 - acc: 0.7632\n",
      "8575/8575 - 0s - loss: 0.4950 - acc: 0.7549\n",
      "Loss: 0.49495953448659824, Accuracy: 0.7548688054084778\n"
     ]
    }
   ],
   "source": [
    "new_nodes_layer1 = 20\n",
    "nodes_layer2 = 8\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "nn.add(tf.keras.layers.Dense(units = new_nodes_layer1, input_dim = input_features, activation = tf.keras.layers.LeakyReLU(alpha=0.1)))\n",
    "nn.add(tf.keras.layers.Dense(units = nodes_layer2, activation = tf.keras.layers.LeakyReLU(alpha=0.1)))\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units =1, activation = \"sigmoid\"))\n",
    "nn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs = 50)\n",
    "#evaluate model accuracy\n",
    "model_loss, model_accuracy =nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random forest predictive accuracy: 0.754\n"
     ]
    }
   ],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistic regression model accuracy: 0.739\n"
     ]
    }
   ],
   "source": [
    "# Define the logistic regression model\n",
    "log_classifier = LogisticRegression(solver=\"lbfgs\",max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "log_classifier.fit(X_train,y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = log_classifier.predict(X_test)\n",
    "print(f\" Logistic regression model accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: It looks like Optimization 4.2 is the best so far, please look at the visualization for this Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
